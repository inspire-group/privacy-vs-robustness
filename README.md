# privacy-vs-robustness
This code accompanies the paper "Privacy Risks of Securing Machine Learning Models against Adversarial Examples" 
https://arxiv.org/abs/1905.10291

We perform membership inference attacks against machine learning models, which are trained to be robust against adversarial examples. 
In total, we evaluate the privacy leakage introduced by six adversarial defense methods, 
and find out that adversarially robust models tend to have more membership information leakage than naturally trained models.

Dependencies: Tensorflow-1.12, Pytorch-0.4
